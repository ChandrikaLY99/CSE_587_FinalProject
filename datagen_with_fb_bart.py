# -*- coding: utf-8 -*-
"""DataGen_With_FB_Bart.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mk6RvQ3IFfIvIfYLwBKSuEKosRh6Hmd5
"""

!pip install arxiv transformers sentencepiece torch pandas tqdm
import arxiv
import pandas as pd
from transformers import pipeline
from tqdm import tqdm
import torch
from datetime import datetime, timedelta

def get_papers_summary():
    search = arxiv.Search(
        query="cat:cs.CL",
        max_results=5000,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    papers = []
    for result in tqdm(search.results(), total=5000, desc="Fetching papers"):
        papers.append({
            'title': result.title,
            'abstract': result.summary,
            'published': result.published,
            'authors': [a.name for a in result.authors],
            'pdf_url': result.pdf_url
        })
        if len(papers) >= 5000:
            break
    return papers

device = 0 if torch.cuda.is_available() else -1
summarizer = pipeline(
    "summarization",
    model="facebook/bart-large-cnn",
    device=device
)

def extract_technical_details(text):
    prompt = f"""
    Extract the following from this research paper abstract:
    1. PROBLEM: Identify the core technical challenge (1-2 sentences)
    2. SOLUTION: Describe the proposed architecture/algorithm (2-3 sentences)

    Text: {text[:1500]}  # Truncate to avoid OOM

    Format exactly as:
    PROBLEM: [description]
    SOLUTION: [description including architecture and algorithms]
    """
    result = summarizer(prompt, max_length=300, min_length=100, do_sample=False)
    return result[0]['summary_text']

def create_enhanced_dataset(papers):
    data = []

    for paper in tqdm(papers, desc="Processing papers"):
        try:
            extracted = extract_technical_details(paper['abstract'])

            problem = extracted.split('PROBLEM:')[-1].split('SOLUTION:')[0].strip()
            solution = extracted.split('SOLUTION:')[-1].strip()

            data.append({
                'title': paper['title'],
                'authors': ', '.join(paper['authors']),
                'date': paper['published'].strftime('%Y-%m-%d'),
                'abstract': paper['abstract'],
                'problem': problem,
                'solution': solution,
                'url': paper['pdf_url']
            })
        except Exception as e:
            print(f"Error processing {paper['title']}: {str(e)}")
            continue

    return pd.DataFrame(data)

if __name__ == "__main__":
    papers = get_papers_summary()
    df = create_enhanced_dataset(papers)

    csv_path = "NLP_Problem_Solution_arxiv.csv"
    df.to_csv(csv_path, index=False)
    print(f"\nSaved {len(df)} papers to {csv_path}")
    print("Sample record:")
    print(df.iloc[0])