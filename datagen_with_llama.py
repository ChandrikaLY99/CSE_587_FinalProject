# -*- coding: utf-8 -*-
"""DataGen_With_Llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtHEQhQNnMAq-aVrFVZRyTkFzfwNCcy8
"""

!pip install -qU transformers accelerate torch sentencepiece arxiv pandas tqdm

!pip uninstall torchvision -y
!pip install torchvision --index-url https://download.pytorch.org/whl/cu118

import arxiv
import pandas as pd
from tqdm import tqdm
from datetime import datetime, timedelta
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from huggingface_hub import login

"""Get only CS.CL papers from arxiv"""

HF_TOKEN = "ACCESS_TOKEN_WITH_LLAMA_PERMISSION"
# (Get from https://huggingface.co/settings/tokens)
login(token=HF_TOKEN)

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

def get_papers():
    search = arxiv.Search(
        query="cat:cs.CL",
        max_results=5000,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )

    papers = []
    for result in tqdm(search.results(), total=5000, desc="Fetching papers"):
        papers.append({
            'title': result.title,
            'abstract': result.summary,
            'published': result.published.strftime('%Y-%m-%d'),
            'authors': ', '.join([a.name for a in result.authors]),
            'pdf_url': result.pdf_url
        })
        if len(papers) >= 5000:
            break
    return papers

if __name__ == "__main__":
    papers = get_papers()
    df = pd.DataFrame(papers)
    df.to_csv("arxiv_cs_cl_papers.csv", index=False)
    print(f"Saved {len(df)} papers to arxiv_cs_cl_papers.csv")
    print("\nSample entry:")
    print(df.head(1).to_string(index=False))

"""Run the LLAMA model to generate the problem and solution from the abstract"""

def extract_with_llama(text):
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
            You are a helpful assistant that extracts structured information from scientific research abstracts.
            Your task is to identify the main PROBLEM the research addresses and summarize the SOLUTION proposed.
            Always include technical details, methods, or techniques used in the SOLUTION if available.<|eot_id|>
            <|start_header_id|>user<|end_header_id|>
            Given the following abstract, extract and label the PROBLEM and the SOLUTION:

            Abstract:
            {text[:2000]}
            <|eot_id|><|start_header_id|>assistant<|end_header_id|>
            PROBLEM:"""

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    model.to(device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=400,
        temperature=0.3,
        do_sample=False,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    problem = full_response.split("PROBLEM:")[-1].split("SOLUTION:")[0].strip()
    solution = full_response.split("SOLUTION:")[-1].strip() if "SOLUTION:" in full_response else "N/A"
    return {"problem": problem, "solution": solution}

"""Generate dataset"""

from tqdm import tqdm
import pandas as pd
from datetime import datetime

def create_dataset(papers):
    data = []
    for _, paper in tqdm(papers.iterrows(), total=len(papers), desc="Processing papers"):
        try:
            abstract = paper['abstract']
            title = paper['title']
            authors = paper['authors']
            date_str = paper['published']
            url = paper['pdf_url']

            authors_formatted = authors if isinstance(authors, str) else ', '.join(authors)
            date_formatted = datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m-%d')

            extracted = extract_with_llama(abstract)

            data.append({
                'title': title,
                'authors': authors_formatted,
                'date': date_formatted,
                'problem': extracted['problem'],
                'solution': extracted['solution'],
                'url': url
            })
        except Exception as e:
            print(f"Error processing {paper.get('title', 'unknown title')}: {str(e)}")
            continue
    return pd.DataFrame(data)

papers = pd.read_csv("/content/arxiv_cs_cl_papers.csv")
df = create_dataset(papers)

csv_path = f"NLP_Problem_Solution_batch.csv"
df.to_csv(csv_path, index=False)
print(f"Saved {len(df)} papers to {csv_path}")
print("Sample output:")
print(df.head(1).to_string(index=False))