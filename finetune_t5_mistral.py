# -*- coding: utf-8 -*-
"""Finetune_T5_Mistral.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I46sR5aGq_50YuVQ8X--_53Y4DemimBc
"""

!pip install -q transformers datasets evaluate peft accelerate bitsandbytes nltk

!pip install rouge_score

!pip install bert_score

from google.colab import files
uploaded = files.upload()

from datasets import load_dataset

dataset = load_dataset('csv', data_files=list(uploaded.keys())[0])['train'].train_test_split(test_size=0.1)
dataset = dataset.rename_columns({"problem": "input_text", "solution": "target_text"})

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments

def preprocess_t5(example, tokenizer):
    model_input = tokenizer(example['input_text'], padding='max_length', truncation=True, max_length=512)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(example['target_text'], padding='max_length', truncation=True, max_length=128)
    model_input["labels"] = labels["input_ids"]
    return model_input

t5_tokenizer = AutoTokenizer.from_pretrained("t5-small")
t5_model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")

t5_tokenized = dataset.map(lambda x: preprocess_t5(x, t5_tokenizer), batched=True)

t5_args = Seq2SeqTrainingArguments(
    output_dir="./t5_finetuned",
    eval_strategy ="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=3e-5,
    num_train_epochs=3,
    predict_with_generate=True,
    logging_dir="./logs_t5",
    report_to="none"
)

t5_trainer = Seq2SeqTrainer(
    model=t5_model,
    args=t5_args,
    train_dataset=t5_tokenized["train"],
    eval_dataset=t5_tokenized["test"],
    tokenizer=t5_tokenizer
)

t5_trainer.train()

from nltk.translate.bleu_score import corpus_bleu
from torch.utils.data import DataLoader
from nltk import word_tokenize
import nltk
import evaluate

import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

def generate_outputs_fast(model, tokenizer, dataset, batch_size=8, max_new_tokens=128):
    preds = []
    refs = []

    # Prepare batches
    inputs = [example["input_text"] for example in dataset]
    refs = [example["target_text"].strip() for example in dataset]

    for i in range(0, len(inputs), batch_size):
        batch_inputs = inputs[i:i+batch_size]

        tokenized = tokenizer(batch_inputs, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)
        outputs = model.generate(**tokenized, max_new_tokens=max_new_tokens)

        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        preds.extend([d.strip() for d in decoded])

    return preds, refs


t5_small_preds, t5_small_refs = generate_outputs_fast(t5_model, t5_tokenizer, dataset["test"])

bleu_t5_small = corpus_bleu(
    [[word_tokenize(ref)] for ref in t5_small_refs],
    [word_tokenize(pred) for pred in t5_small_preds]
)

rouge_result = rouge.compute(predictions=t5_small_preds, references=t5_small_refs)

bertscore_result = bertscore.compute(predictions=t5_small_preds, references=t5_small_refs, lang="en")

print("5-Small Evaluation Metrics:")
print(f"- BLEU Score:   {bleu_t5_small:.4f}")
print(f"- ROUGE-1:      {rouge_result['rouge1']:.4f}")
print(f"- ROUGE-2:      {rouge_result['rouge2']:.4f}")
print(f"- ROUGE-L:      {rouge_result['rougeL']:.4f}")
print(f"- BERTScore F1: {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}")

print("\n Sample Predictions from T5-Small:")
for i in range(5):
    print(f"Problem: {dataset['test'][i]['input_text']}")
    print(f"Target:  {t5_small_refs[i]}")
    print(f"Pred:    {t5_small_preds[i]}")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments
import evaluate
import torch
import nltk

nltk.download('punkt')
nltk.download('punkt_tab')

t5_base_model_id = "t5-base"
t5_base_tokenizer = AutoTokenizer.from_pretrained(t5_base_model_id)
t5_base_model = AutoModelForSeq2SeqLM.from_pretrained(t5_base_model_id).to("cuda")

def preprocess_t5_base(example):
    inputs = t5_base_tokenizer(example['input_text'], padding='max_length', truncation=True, max_length=512)
    targets = t5_base_tokenizer(example['target_text'], padding='max_length', truncation=True, max_length=128)
    inputs["labels"] = targets["input_ids"]
    return inputs

t5_base_tokenized = dataset.map(preprocess_t5_base, batched=True)

t5_base_args = Seq2SeqTrainingArguments(
    output_dir="./t5_base_finetuned",
    eval_strategy="epoch",
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    learning_rate=3e-5,
    num_train_epochs=3,
    predict_with_generate=True,
    logging_dir="./logs_t5_base",
    report_to="none"
)

t5_base_trainer = Seq2SeqTrainer(
    model=t5_base_model,
    args=t5_base_args,
    train_dataset=t5_base_tokenized["train"],
    eval_dataset=t5_base_tokenized["test"],
    tokenizer=t5_base_tokenizer
)

t5_base_trainer.train()

from nltk.translate.bleu_score import corpus_bleu
from nltk import word_tokenize
import evaluate

rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

def generate_outputs_t5_base(model, tokenizer, dataset, batch_size=8, max_new_tokens=128):
    preds = []
    refs = []
    inputs = [ex["input_text"] for ex in dataset]
    refs = [ex["target_text"].strip() for ex in dataset]

    for i in range(0, len(inputs), batch_size):
        batch_inputs = inputs[i:i+batch_size]
        tokenized = tokenizer(batch_inputs, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)

        with torch.no_grad():
            outputs = model.generate(**tokenized, max_new_tokens=max_new_tokens)

        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        preds.extend([d.strip() for d in decoded])

    return preds, refs

t5_base_model.eval()
t5_base_preds, t5_base_refs = generate_outputs_t5_base(t5_base_model, t5_base_tokenizer, dataset["test"])

bleu_base = corpus_bleu([[word_tokenize(r)] for r in t5_base_refs],
                        [word_tokenize(p) for p in t5_base_preds])

rouge_base = rouge.compute(predictions=t5_base_preds, references=t5_base_refs)
bertscore_base = bertscore.compute(predictions=t5_base_preds, references=t5_base_refs, lang="en")

print("T5-Base Evaluation Metrics:")
print(f"- BLEU Score:   {bleu_base:.4f}")
print(f"- ROUGE-1:      {rouge_base['rouge1']:.4f}")
print(f"- ROUGE-2:      {rouge_base['rouge2']:.4f}")
print(f"- ROUGE-L:      {rouge_base['rougeL']:.4f}")
print(f"- BERTScore F1: {sum(bertscore_base['f1']) / len(bertscore_base['f1']):.4f}")

print("\n Sample Predictions from T5-Base:")
for i in range(5):
    print(f"\nProblem: {dataset['test'][i]['input_text']}")
    print(f"Target:  {t5_base_refs[i]}")
    print(f"Pred:    {t5_base_preds[i]}")

from huggingface_hub import login

login()

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import get_peft_model, LoraConfig, TaskType
import torch

mistral_model_id = "mistralai/Mistral-7B-Instruct-v0.2"

mistral_tokenizer = AutoTokenizer.from_pretrained(mistral_model_id)
mistral_tokenizer.pad_token = mistral_tokenizer.eos_token
mistral_model = AutoModelForCausalLM.from_pretrained(
    mistral_model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

def format_prompt(example):
    return {
        "input": f"### Problem:\n{example['input_text']}\n\n### Solution:\n{example['target_text']}"
    }

mistral_dataset = dataset.map(format_prompt)
mistral_dataset = mistral_dataset.rename_column("input", "text")

def tokenize(example):
    return mistral_tokenizer(example["text"], padding="max_length", truncation=True, max_length=512)

tokenized_mistral = mistral_dataset.map(tokenize, batched=True)

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM
)

mistral_model = get_peft_model(mistral_model, lora_config)

mistral_args = TrainingArguments(
    output_dir="./mistral_finetuned",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    learning_rate=2e-5,
    num_train_epochs=3,
    fp16=True,
    eval_strategy="epoch",
    save_strategy="epoch",
    report_to="none"
)

trainer = Trainer(
    model=mistral_model,
    args=mistral_args,
    train_dataset=tokenized_mistral["train"],
    eval_dataset=tokenized_mistral["test"],
    tokenizer=mistral_tokenizer,
    data_collator=DataCollatorForLanguageModeling(tokenizer=mistral_tokenizer, mlm=False)
)

trainer.train()

import nltk
import evaluate
from nltk.translate.bleu_score import corpus_bleu
from nltk import word_tokenize

nltk.download('punkt')

def generate_outputs_mistral(model, tokenizer, dataset, batch_size=4, max_new_tokens=128):
    model.eval()
    preds = []
    refs = []
    inputs = [f"### Problem:\n{x['input_text']}\n\n### Solution:\n" for x in dataset]
    refs = [x["target_text"].strip() for x in dataset]

    for i in range(0, len(inputs), batch_size):
        batch_inputs = inputs[i:i+batch_size]
        tokenized = tokenizer(batch_inputs, return_tensors="pt", padding=True, truncation=True, max_length=512).to(model.device)

        with torch.no_grad():
            outputs = model.generate(**tokenized, max_new_tokens=max_new_tokens)

        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        batch_preds = [out.split("### Solution:")[-1].strip() for out in decoded]
        preds.extend(batch_preds)

    return preds, refs

rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

mistral_preds, mistral_refs = generate_outputs_mistral(mistral_model, mistral_tokenizer, dataset["test"])

bleu_mistral = corpus_bleu([[word_tokenize(r)] for r in mistral_refs],
                           [word_tokenize(p) for p in mistral_preds])

rouge_result = rouge.compute(predictions=mistral_preds, references=mistral_refs)

bertscore_result = bertscore.compute(predictions=mistral_preds, references=mistral_refs, lang="en")

print("Mistral Evaluation Metrics:")
print(f"- BLEU Score:   {bleu_mistral:.4f}")
print(f"- ROUGE-1:      {rouge_result['rouge1']:.4f}")
print(f"- ROUGE-2:      {rouge_result['rouge2']:.4f}")
print(f"- ROUGE-L:      {rouge_result['rougeL']:.4f}")
print(f"- BERTScore F1: {sum(bertscore_result['f1']) / len(bertscore_result['f1']):.4f}")

for i in range(5):
    print(f"Problem: {dataset['test'][i]['input_text']}")
    print(f"Target:  {mistral_refs[i]}")
    print(f"Mistral: {mistral_preds[i]}")